apiVersion: v1
kind: Service
metadata:
  name: qwen3-30b-a3b
  labels:
    app: qwen3-30b-a3b
spec:
  selector:
    app: qwen3-30b-a3b
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen3-30b-a3b
  labels:
    app: qwen3-30b-a3b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: qwen3-30b-a3b
  template:
    metadata:
      labels:
        app: qwen3-30b-a3b
    spec:
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "8Gi"
        - name: localtime
          hostPath:
            path: /etc/localtime
            type: File
        - name: qwen-model
          hostPath:
            path: /data/models/
            type: Directory
      containers:
        - name: qwen3-30b-a3b
          image: vllm/vllm-openai:v0.11.0
          env:
            - name: TZ
              value: Asia/Shanghai
            - name: NCCL_DEBUG
              value: INFO
            - name: NCCL_P2P_DISABLE
              value: "0"
            - name: NCCL_SHM_DISABLE
              value: "1"
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1"
            - name: OMP_NUM_THREADS
              value: "1"
          command: ["/bin/sh", "-c"]
          args: [
            "vllm serve /models/Qwen3-30B-A3B \
                --served_model_name Qwen3-30B-A3B \
                --max-model-len=40960 \
                --enable-expert-parallel \
                --enable-auto-tool-choice \
                --tool-call-parser=hermes \
                --tensor-parallel-size 2 \
                --pipeline-parallel-size 1 \
                --gpu-memory-utilization 0.75 \
                --api-key sk-7JpVa2uSQkHLeKMMFsRxgBhDrpX9cB1NRmC9xVPrfAdLIm4F \
                --host 0.0.0.0 \
                --port 8000"
          ]
          ports:
            - containerPort: 8000
          resources:
            limits:
              cpu: "10"
              memory: 50G
              nvidia.com/gpu: "2"
            requests:
              cpu: "2"
              memory: 6G
              nvidia.com/gpu: "2"
          volumeMounts:
            - name: qwen-model
              mountPath: /models/
              readOnly: true
            - name: localtime
              mountPath: /etc/localtime
              readOnly: true
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 5
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3